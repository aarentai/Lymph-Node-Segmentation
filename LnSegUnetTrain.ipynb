{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dataset import *\n",
    "from model import *\n",
    "from loss import *\n",
    "import os\n",
    "import SimpleITK as sitk\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode='gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode=='gpu':\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # after switch device, you need restart the kernel\n",
    "#     torch.cuda.set_device(1)\n",
    "    torch.set_default_tensor_type('torch.cuda.DoubleTensor')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For classifications(segmentation=voxel-wise classification), `F.softmax(output, dim=1)` is very necessary at the end of the model, as it constraints the output into a probability, or you may have negative value that you also have no clue where it comes from.\n",
    "2. The numerator in dice loss for each category is very much like the cross entropy: a softmax vector inner product with a one-hot vector - only the value at where one is matters.\n",
    "2. For segmentation, use dice loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resume:True, save_model:True\n"
     ]
    }
   ],
   "source": [
    "resume = True\n",
    "save_model = True\n",
    "print(f'resume:{resume}, save_model:{save_model}')\n",
    "output_dir = 'Models/Unet'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params 464849, # conv layers 30\n",
      "Starting from iteration 5 to iteration 1006\n"
     ]
    }
   ],
   "source": [
    "epoch_loss_list = []\n",
    "epoch_num = 1001\n",
    "start_epoch_num = 5\n",
    "batch_size = 1\n",
    "learning_rate = 15\n",
    "\n",
    "model = UNet64()\n",
    "model.train()\n",
    "if mode=='gpu':\n",
    "    model.cuda()\n",
    "net = torch.nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n",
    "# criterion = DiceLoss()\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "\n",
    "dataset = UnetDataset(root_dir='/home/sci/hdai/Projects/Dataset/LymphNodes')\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "if resume:\n",
    "    checkpoint = torch.load(f'{output_dir}/epoch_{start_epoch_num-1}_checkpoint.pth.tar')    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    with open(f'{output_dir}/loss.txt', 'a') as f:\n",
    "        f.write(f'From {start_epoch_num} to {epoch_num+start_epoch_num}\\n')\n",
    "        f.write(f'Dice; Adadelta, lr={learning_rate}; batch size: {batch_size}\\n')\n",
    "else:\n",
    "    start_epoch_num = 0  \n",
    "    \n",
    "    with open(f'{output_dir}/loss.txt', 'w+') as f:\n",
    "        f.write(f'From {start_epoch_num} to {epoch_num+start_epoch_num}\\n')\n",
    "        f.write(f'Dice; Adadelta: lr={learning_rate}; batch size: {batch_size}\\n')\n",
    "    \n",
    "print(f'Starting from iteration {start_epoch_num} to iteration {epoch_num+start_epoch_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1001 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:15, 15.38s/it]\u001b[A\n",
      "2it [00:33, 16.99s/it]\u001b[A\n",
      "3it [00:51, 17.55s/it]\u001b[A\n",
      "4it [01:09, 17.81s/it]\u001b[A\n",
      "5it [01:23, 16.23s/it]\u001b[A\n",
      "6it [01:37, 15.45s/it]\u001b[A\n",
      "7it [01:52, 15.50s/it]\u001b[A\n",
      "8it [02:09, 15.82s/it]\u001b[A\n",
      "9it [02:23, 15.28s/it]\u001b[A\n",
      "10it [02:37, 14.86s/it]\u001b[A\n",
      "11it [02:53, 15.37s/it]\u001b[A\n",
      "12it [03:07, 14.79s/it]\u001b[A\n",
      "13it [03:21, 14.59s/it]\u001b[A\n",
      "14it [03:35, 14.43s/it]\u001b[A\n",
      "15it [03:54, 15.77s/it]\u001b[A\n",
      "16it [04:12, 16.31s/it]\u001b[A\n",
      "17it [04:37, 18.93s/it]\u001b[A\n",
      "18it [04:50, 17.14s/it]\u001b[A\n",
      "19it [05:15, 19.69s/it]\u001b[A\n",
      "20it [05:30, 18.10s/it]\u001b[A\n",
      "21it [05:45, 17.36s/it]\u001b[A\n",
      "22it [06:02, 17.26s/it]\u001b[A\n",
      "23it [06:20, 17.40s/it]\u001b[A\n",
      "24it [06:32, 15.84s/it]\u001b[A\n",
      "25it [06:45, 15.04s/it]\u001b[A\n",
      "26it [06:59, 14.78s/it]\u001b[A\n",
      "27it [07:13, 14.31s/it]\u001b[A\n",
      "28it [07:25, 13.66s/it]\u001b[A\n",
      "29it [07:51, 17.27s/it]\u001b[A\n",
      "30it [08:04, 16.17s/it]\u001b[A\n",
      "31it [08:21, 16.42s/it]\u001b[A\n",
      "32it [08:39, 16.89s/it]\u001b[A\n",
      "33it [08:57, 17.18s/it]\u001b[A\n",
      "34it [09:10, 16.01s/it]\u001b[A\n",
      "35it [09:25, 15.57s/it]\u001b[A\n",
      "36it [09:39, 15.11s/it]\u001b[A\n",
      "37it [09:53, 14.82s/it]\u001b[A\n",
      "38it [10:11, 15.72s/it]\u001b[A\n",
      "39it [10:25, 15.23s/it]\u001b[A\n",
      "40it [10:40, 15.31s/it]\u001b[A\n",
      "41it [10:59, 16.20s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(start_epoch_num, start_epoch_num+epoch_num)):\n",
    "    epoch_loss = 0\n",
    "            \n",
    "    for i, batched_sample in tqdm(enumerate(dataloader)):\n",
    "        '''innerdomain backpropagate'''\n",
    "#         print(i)\n",
    "        input_data = batched_sample['img'].double()#.to(device)\n",
    "#         print(input.shape)\n",
    "        input_data.requires_grad = True\n",
    "        # u_pred: [batch_size, *data_shape, feature_num] = [1, 5, ...]\n",
    "        output_pred = net(input_data)\n",
    "        output_true = batched_sample['mask']#.to(device)#.double()\n",
    "#         print(output_pred.shape, output_true.shape)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "#         loss = criterion(output_pred, output_true.squeeze())\n",
    "        loss = criterion(output_pred, output_true.double())\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    with open(f'{output_dir}/loss.txt', 'a') as f:\n",
    "        f.write(f'{epoch_loss}\\n')\n",
    "    \n",
    "    print(f'epoch {epoch} innerdomain loss: {epoch_loss}')#, norm: {torch.norm(f_pred,2)**2}\n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    if epoch%1==0:       \n",
    "        if save_model:\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'optimizer_bd_state_dict': optimizer_bd.state_dict(),\n",
    "            'loss': epoch_loss,\n",
    "#             'loss_bd': epoch_loss_bd\n",
    "            }, f'{output_dir}/epoch_{epoch}_checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred.device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_id.shape)\n",
    "print(output_pred_id.shape, output_true_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_true_id.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Innerdomain loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.plot(epoch_loss_list)\n",
    "plt.savefig(f'{output_dir}/adadelta_loss_1e-1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch17",
   "language": "python",
   "name": "pytorch17"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
